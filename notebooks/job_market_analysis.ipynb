{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Job Market Analysis Tool\n",
        "\n",
        "## Data-Driven Analysis of Tech Job Market\n",
        "\n",
        "**Author:** Computer Science Graduate  \n",
        "**Date:** November 2025  \n",
        "**Purpose:** Analyze job market trends, in-demand skills, and salary patterns\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Table of Contents\n",
        "1. [Setup & Imports](#setup)\n",
        "2. [Data Collection](#data-collection)\n",
        "3. [Data Cleaning & Preprocessing](#data-cleaning)\n",
        "4. [Exploratory Data Analysis](#eda)\n",
        "5. [Skill Extraction & Analysis](#skill-extraction)\n",
        "6. [Data Visualization](#visualization)\n",
        "7. [Machine Learning - Job Clustering](#clustering)\n",
        "8. [Key Insights & Recommendations](#insights)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports <a id='setup'></a>\n",
        "\n",
        "Import necessary libraries for data processing, visualization, and analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Natural Language Processing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Utilities\n",
        "from collections import Counter\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"üìä Pandas Version: {pd.__version__}\")\n",
        "print(f\"üî¢ NumPy Version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download NLTK data (run once)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "    print(\"‚úÖ NLTK data already downloaded\")\n",
        "except LookupError:\n",
        "    print(\"üì• Downloading NLTK data...\")\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    print(\"‚úÖ NLTK data downloaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Collection <a id='data-collection'></a>\n",
        "\n",
        "### Dataset Information\n",
        "\n",
        "**Recommended Dataset:** [LinkedIn Job Postings - 2023](https://www.kaggle.com/datasets/arshkon/linkedin-job-postings)\n",
        "\n",
        "**Instructions:**\n",
        "1. Download the dataset from Kaggle\n",
        "2. Place the CSV file in `../data/raw/` folder\n",
        "3. Update the file path below\n",
        "\n",
        "**Alternative datasets:**\n",
        "- [Data Science Job Postings](https://www.kaggle.com/datasets/rashikrahmanpritom/data-science-job-posting-on-glassdoor)\n",
        "- [Job Posts Data](https://www.kaggle.com/datasets/madhab/jobposts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "# Update this path to match your downloaded dataset file name\n",
        "data_path = '../data/raw/linkedin_job_postings.csv'\n",
        "\n",
        "# Alternative: if you have a different dataset, update the path\n",
        "# data_path = '../data/raw/job_postings.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "    print(f\"üìä Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ö†Ô∏è Dataset not found!\")\n",
        "    print(f\"Please download a job postings dataset and place it at: {data_path}\")\n",
        "    print(\"\\nRecommended: https://www.kaggle.com/datasets/arshkon/linkedin-job-postings\")\n",
        "    \n",
        "    # Create sample data for demonstration purposes\n",
        "    print(\"\\nüîÑ Creating sample dataset for demonstration...\")\n",
        "    df = pd.DataFrame({\n",
        "        'job_title': ['Data Scientist', 'Software Engineer', 'ML Engineer', 'Data Analyst', 'Full Stack Developer'] * 200,\n",
        "        'company': ['Company A', 'Company B', 'Company C', 'Company D', 'Company E'] * 200,\n",
        "        'location': ['Riyadh, Saudi Arabia', 'Jeddah, Saudi Arabia', 'Dubai, UAE', 'Remote', 'Dammam, Saudi Arabia'] * 200,\n",
        "        'description': [\n",
        "            'Looking for Data Scientist with Python, SQL, Machine Learning, TensorFlow experience',\n",
        "            'Software Engineer needed. Java, Spring Boot, AWS, Docker required',\n",
        "            'ML Engineer position. Python, PyTorch, Kubernetes, MLOps skills needed',\n",
        "            'Data Analyst role. SQL, Tableau, Excel, Power BI required',\n",
        "            'Full Stack Developer. React, Node.js, MongoDB, JavaScript expertise needed'\n",
        "        ] * 200,\n",
        "        'salary': np.random.choice([None, '80000-120000', '100000-150000', '60000-90000'], 1000),\n",
        "        'experience_level': np.random.choice(['Entry Level', 'Mid Level', 'Senior Level', 'Lead'], 1000),\n",
        "        'posted_date': pd.date_range('2023-01-01', periods=1000, freq='D')\n",
        "    })\n",
        "    print(f\"‚úÖ Sample dataset created: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display basic information\n",
        "print(\"=\" * 80)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\" * 80)\n",
        "df.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset information\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATASET INFO\")\n",
        "print(\"=\" * 80)\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"MISSING VALUES\")\n",
        "print(\"=\" * 80)\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_values.index,\n",
        "    'Missing Count': missing_values.values,\n",
        "    'Percentage': missing_percentage.values\n",
        "}).sort_values('Missing Count', ascending=False)\n",
        "\n",
        "print(missing_df[missing_df['Missing Count'] > 0].to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy for cleaning\n",
        "df_clean = df.copy()\n",
        "\n",
        "print(\"üßπ Starting data cleaning process...\\n\")\n",
        "\n",
        "# 1. Remove duplicates\n",
        "initial_rows = len(df_clean)\n",
        "df_clean = df_clean.drop_duplicates()\n",
        "duplicates_removed = initial_rows - len(df_clean)\n",
        "print(f\"‚úÖ Removed {duplicates_removed:,} duplicate rows\")\n",
        "\n",
        "# 2. Handle missing values in critical columns\n",
        "# For job title and description (critical fields)\n",
        "if 'job_title' in df_clean.columns:\n",
        "    df_clean = df_clean.dropna(subset=['job_title'])\n",
        "    \n",
        "if 'description' in df_clean.columns:\n",
        "    df_clean['description'] = df_clean['description'].fillna('')\n",
        "    \n",
        "# For other columns, fill with 'Unknown' or appropriate value\n",
        "for col in df_clean.columns:\n",
        "    if df_clean[col].dtype == 'object':\n",
        "        df_clean[col] = df_clean[col].fillna('Unknown')\n",
        "\n",
        "print(f\"‚úÖ Handled missing values\")\n",
        "\n",
        "# 3. Standardize text columns (lowercase, strip whitespace)\n",
        "text_columns = ['job_title', 'company', 'location', 'description']\n",
        "for col in text_columns:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = df_clean[col].astype(str).str.strip()\n",
        "\n",
        "print(f\"‚úÖ Standardized text columns\")\n",
        "\n",
        "# 4. Clean job titles (normalize similar titles)\n",
        "if 'job_title' in df_clean.columns:\n",
        "    df_clean['job_title_clean'] = df_clean['job_title'].str.lower()\n",
        "    \n",
        "    # Normalize common variations\n",
        "    title_mapping = {\n",
        "        r'data scientist.*': 'Data Scientist',\n",
        "        r'machine learning.*': 'Machine Learning Engineer',\n",
        "        r'ml engineer.*': 'Machine Learning Engineer',\n",
        "        r'software engineer.*': 'Software Engineer',\n",
        "        r'software developer.*': 'Software Engineer',\n",
        "        r'data analyst.*': 'Data Analyst',\n",
        "        r'data engineer.*': 'Data Engineer',\n",
        "        r'full.?stack.*': 'Full Stack Developer',\n",
        "        r'frontend.*|front.end.*': 'Frontend Developer',\n",
        "        r'backend.*|back.end.*': 'Backend Developer',\n",
        "        r'devops.*': 'DevOps Engineer',\n",
        "        r'cloud.*engineer.*': 'Cloud Engineer',\n",
        "        r'product manager.*': 'Product Manager',\n",
        "        r'business analyst.*': 'Business Analyst'\n",
        "    }\n",
        "    \n",
        "    for pattern, normalized_title in title_mapping.items():\n",
        "        df_clean.loc[df_clean['job_title_clean'].str.contains(pattern, case=False, na=False), \n",
        "                     'job_title_clean'] = normalized_title\n",
        "    \n",
        "    print(f\"‚úÖ Normalized job titles\")\n",
        "\n",
        "print(f\"\\n‚úÖ Data cleaning complete!\")\n",
        "print(f\"üìä Final dataset: {len(df_clean):,} rows √ó {len(df_clean.columns)} columns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display cleaned data\n",
        "df_clean.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summary\n",
        "print(\"=\" * 80)\n",
        "print(\"STATISTICAL SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "df_clean.describe(include='all')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top job titles\n",
        "if 'job_title_clean' in df_clean.columns:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TOP 15 JOB TITLES\")\n",
        "    print(\"=\" * 80)\n",
        "    top_jobs = df_clean['job_title_clean'].value_counts().head(15)\n",
        "    print(top_jobs.to_string())\n",
        "    \n",
        "    # Visualization\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    top_jobs.plot(kind='barh', ax=ax, color='steelblue')\n",
        "    ax.set_xlabel('Number of Job Postings', fontsize=12)\n",
        "    ax.set_ylabel('Job Title', fontsize=12)\n",
        "    ax.set_title('Top 15 Most Common Job Titles', fontsize=14, fontweight='bold')\n",
        "    ax.invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../visualizations/top_job_titles.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è job_title_clean column not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Location distribution\n",
        "if 'location' in df_clean.columns:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TOP LOCATIONS\")\n",
        "    print(\"=\" * 80)\n",
        "    top_locations = df_clean['location'].value_counts().head(10)\n",
        "    print(top_locations.to_string())\n",
        "    \n",
        "    # Visualization\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    top_locations.plot(kind='bar', ax=ax, color='coral')\n",
        "    ax.set_xlabel('Location', fontsize=12)\n",
        "    ax.set_ylabel('Number of Job Postings', fontsize=12)\n",
        "    ax.set_title('Top 10 Locations with Most Job Postings', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../visualizations/top_locations.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experience level distribution\n",
        "if 'experience_level' in df_clean.columns:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"EXPERIENCE LEVEL DISTRIBUTION\")\n",
        "    print(\"=\" * 80)\n",
        "    exp_dist = df_clean['experience_level'].value_counts()\n",
        "    print(exp_dist.to_string())\n",
        "    \n",
        "    # Visualization - Pie chart\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']\n",
        "    exp_dist.plot(kind='pie', ax=ax, autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "    ax.set_ylabel('')\n",
        "    ax.set_title('Distribution of Jobs by Experience Level', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../visualizations/experience_distribution.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Skill Extraction & Analysis <a id='skill-extraction'></a>\n",
        "\n",
        "Extract and analyze technical skills from job descriptions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define comprehensive skills dictionary\n",
        "SKILLS_DICT = {\n",
        "    # Programming Languages\n",
        "    'Programming Languages': [\n",
        "        'python', 'java', 'javascript', 'typescript', 'c\\\\+\\\\+', 'c#', 'csharp',\n",
        "        'r\\\\b', 'matlab', 'scala', 'kotlin', 'swift', 'go\\\\b', 'golang', 'rust',\n",
        "        'php', 'ruby', 'perl', 'julia', 'bash', 'shell'\n",
        "    ],\n",
        "    \n",
        "    # Data Science & ML\n",
        "    'Data Science & ML': [\n",
        "        'machine learning', 'deep learning', 'neural network', 'tensorflow', \n",
        "        'pytorch', 'keras', 'scikit-learn', 'sklearn', 'pandas', 'numpy', \n",
        "        'matplotlib', 'seaborn', 'nlp', 'computer vision', 'opencv', 'hugging face',\n",
        "        'transformers', 'bert', 'gpt', 'llm', 'artificial intelligence', 'ai\\\\b'\n",
        "    ],\n",
        "    \n",
        "    # Databases\n",
        "    'Databases': [\n",
        "        'sql', 'mysql', 'postgresql', 'mongodb', 'redis', 'cassandra', \n",
        "        'oracle', 'sqlite', 'mssql', 'dynamodb', 'elasticsearch', 'neo4j',\n",
        "        'mariadb', 'firebase', 'cosmos'\n",
        "    ],\n",
        "    \n",
        "    # Cloud Platforms\n",
        "    'Cloud Platforms': [\n",
        "        'aws', 'azure', 'gcp', 'google cloud', 'cloud computing', 'lambda',\n",
        "        's3\\\\b', 'ec2', 'kubernetes', 'docker', 'containerization', 'microservices'\n",
        "    ],\n",
        "    \n",
        "    # Web Development\n",
        "    'Web Development': [\n",
        "        'react', 'angular', 'vue\\\\.js', 'node\\\\.js', 'express', 'django',\n",
        "        'flask', 'spring', 'spring boot', 'asp\\\\.net', 'html', 'css', \n",
        "        'rest api', 'graphql', 'webpack', 'redux'\n",
        "    ],\n",
        "    \n",
        "    # DevOps & Tools\n",
        "    'DevOps & Tools': [\n",
        "        'git', 'github', 'gitlab', 'jenkins', 'ci/cd', 'terraform', \n",
        "        'ansible', 'puppet', 'chef', 'circleci', 'travis', 'devops'\n",
        "    ],\n",
        "    \n",
        "    # Big Data\n",
        "    'Big Data': [\n",
        "        'hadoop', 'spark', 'kafka', 'airflow', 'hive', 'pig', 'flink',\n",
        "        'storm', 'etl', 'data pipeline', 'data warehouse', 'snowflake'\n",
        "    ],\n",
        "    \n",
        "    # BI & Visualization\n",
        "    'BI & Visualization': [\n",
        "        'tableau', 'power bi', 'powerbi', 'looker', 'qlik', 'excel',\n",
        "        'd3\\\\.js', 'plotly', 'dashboarding', 'data visualization'\n",
        "    ],\n",
        "    \n",
        "    # Soft Skills\n",
        "    'Soft Skills': [\n",
        "        'agile', 'scrum', 'leadership', 'communication', 'teamwork',\n",
        "        'problem solving', 'analytical', 'project management'\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Skills dictionary created with\", sum(len(v) for v in SKILLS_DICT.values()), \"skill patterns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to extract skills from text\n",
        "def extract_skills(text, skills_dict):\n",
        "    \"\"\"Extract skills from job description text.\"\"\"\n",
        "    if pd.isna(text) or text == '':\n",
        "        return []\n",
        "    \n",
        "    text_lower = str(text).lower()\n",
        "    found_skills = []\n",
        "    \n",
        "    for category, skills in skills_dict.items():\n",
        "        for skill in skills:\n",
        "            # Use regex for more accurate matching\n",
        "            pattern = r'\\b' + skill + r'\\b'\n",
        "            if re.search(pattern, text_lower, re.IGNORECASE):\n",
        "                # Store the skill in a cleaner format\n",
        "                skill_name = skill.replace('\\\\b', '').replace('\\\\+\\\\+', '++').replace('\\\\.', '.')\n",
        "                found_skills.append(skill_name)\n",
        "    \n",
        "    return found_skills\n",
        "\n",
        "# Apply skill extraction\n",
        "print(\"üîç Extracting skills from job descriptions...\")\n",
        "if 'description' in df_clean.columns:\n",
        "    df_clean['skills'] = df_clean['description'].apply(lambda x: extract_skills(x, SKILLS_DICT))\n",
        "    df_clean['skills_count'] = df_clean['skills'].apply(len)\n",
        "    print(f\"‚úÖ Skills extracted! Average {df_clean['skills_count'].mean():.1f} skills per job\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Description column not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze most in-demand skills\n",
        "if 'skills' in df_clean.columns:\n",
        "    # Flatten all skills into a single list\n",
        "    all_skills = [skill for skills_list in df_clean['skills'] for skill in skills_list]\n",
        "    \n",
        "    # Count skill frequency\n",
        "    skill_counts = Counter(all_skills)\n",
        "    top_skills = pd.DataFrame(skill_counts.most_common(30), columns=['Skill', 'Count'])\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TOP 30 MOST IN-DEMAND SKILLS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(top_skills.to_string(index=False))\n",
        "    \n",
        "    # Visualize top 20 skills\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    top_20 = top_skills.head(20)\n",
        "    ax.barh(range(len(top_20)), top_20['Count'], color='mediumseagreen')\n",
        "    ax.set_yticks(range(len(top_20)))\n",
        "    ax.set_yticklabels(top_20['Skill'])\n",
        "    ax.set_xlabel('Number of Job Postings', fontsize=12)\n",
        "    ax.set_ylabel('Skill', fontsize=12)\n",
        "    ax.set_title('Top 20 Most In-Demand Skills', fontsize=14, fontweight='bold')\n",
        "    ax.invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../visualizations/top_skills.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Skills by category analysis\n",
        "if 'skills' in df_clean.columns:\n",
        "    category_skills = {category: [] for category in SKILLS_DICT.keys()}\n",
        "    \n",
        "    for skills_list in df_clean['skills']:\n",
        "        for skill in skills_list:\n",
        "            for category, skill_patterns in SKILLS_DICT.items():\n",
        "                for pattern in skill_patterns:\n",
        "                    clean_pattern = pattern.replace('\\\\b', '').replace('\\\\+\\\\+', '++').replace('\\\\.', '.')\n",
        "                    if skill == clean_pattern:\n",
        "                        category_skills[category].append(skill)\n",
        "                        break\n",
        "    \n",
        "    # Count skills by category\n",
        "    category_counts = {cat: len(skills) for cat, skills in category_skills.items()}\n",
        "    category_df = pd.DataFrame(list(category_counts.items()), \n",
        "                               columns=['Category', 'Count']).sort_values('Count', ascending=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SKILLS BY CATEGORY\")\n",
        "    print(\"=\" * 80)\n",
        "    print(category_df.to_string(index=False))\n",
        "    \n",
        "    # Visualization\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.bar(range(len(category_df)), category_df['Count'], color='skyblue', edgecolor='navy')\n",
        "    ax.set_xticks(range(len(category_df)))\n",
        "    ax.set_xticklabels(category_df['Category'], rotation=45, ha='right')\n",
        "    ax.set_xlabel('Skill Category', fontsize=12)\n",
        "    ax.set_ylabel('Total Mentions', fontsize=12)\n",
        "    ax.set_title('Distribution of Skills by Category', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../visualizations/skills_by_category.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Visualization <a id='visualization'></a>\n",
        "\n",
        "Create comprehensive visualizations and insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word Cloud of Skills\n",
        "if 'skills' in df_clean.columns and len(all_skills) > 0:\n",
        "    print(\"‚òÅÔ∏è Generating word cloud...\")\n",
        "    \n",
        "    # Create text from all skills\n",
        "    skills_text = ' '.join(all_skills)\n",
        "    \n",
        "    # Generate word cloud\n",
        "    wordcloud = WordCloud(\n",
        "        width=1600, \n",
        "        height=800, \n",
        "        background_color='white',\n",
        "        colormap='viridis',\n",
        "        relative_scaling=0.5,\n",
        "        min_font_size=10\n",
        "    ).generate(skills_text)\n",
        "    \n",
        "    # Display\n",
        "    fig, ax = plt.subplots(figsize=(16, 8))\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.axis('off')\n",
        "    ax.set_title('Skills Word Cloud - Most In-Demand Technologies', \n",
        "                 fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../visualizations/skills_wordcloud.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"‚úÖ Word cloud generated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Skills required by top job titles\n",
        "if 'skills' in df_clean.columns and 'job_title_clean' in df_clean.columns:\n",
        "    print(\"\\nüìä Analyzing skills by job title...\")\n",
        "    \n",
        "    # Get top 5 job titles\n",
        "    top_5_jobs = df_clean['job_title_clean'].value_counts().head(5).index\n",
        "    \n",
        "    # Create skill frequency for each job title\n",
        "    job_skills_data = []\n",
        "    \n",
        "    for job in top_5_jobs:\n",
        "        job_df = df_clean[df_clean['job_title_clean'] == job]\n",
        "        job_skills = [skill for skills_list in job_df['skills'] for skill in skills_list]\n",
        "        \n",
        "        if len(job_skills) > 0:\n",
        "            skill_counts = Counter(job_skills).most_common(10)\n",
        "            for skill, count in skill_counts:\n",
        "                job_skills_data.append({\n",
        "                    'Job Title': job,\n",
        "                    'Skill': skill,\n",
        "                    'Count': count\n",
        "                })\n",
        "    \n",
        "    if len(job_skills_data) > 0:\n",
        "        job_skills_df = pd.DataFrame(job_skills_data)\n",
        "        \n",
        "        # Create grouped bar chart\n",
        "        fig, axes = plt.subplots(len(top_5_jobs), 1, figsize=(14, 4*len(top_5_jobs)))\n",
        "        \n",
        "        if len(top_5_jobs) == 1:\n",
        "            axes = [axes]\n",
        "        \n",
        "        for idx, job in enumerate(top_5_jobs):\n",
        "            job_data = job_skills_df[job_skills_df['Job Title'] == job].sort_values('Count', ascending=True)\n",
        "            \n",
        "            if len(job_data) > 0:\n",
        "                axes[idx].barh(job_data['Skill'], job_data['Count'], color=f'C{idx}')\n",
        "                axes[idx].set_title(f'Top Skills for {job}', fontsize=12, fontweight='bold')\n",
        "                axes[idx].set_xlabel('Frequency')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig('../visualizations/skills_by_job_title.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(\"‚úÖ Skills by job title visualized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Skill co-occurrence heatmap (which skills appear together)\n",
        "if 'skills' in df_clean.columns:\n",
        "    print(\"\\nüî• Creating skill co-occurrence heatmap...\")\n",
        "    \n",
        "    # Get top 15 skills\n",
        "    top_skills_list = [skill for skill, count in skill_counts.most_common(15)]\n",
        "    \n",
        "    # Create co-occurrence matrix\n",
        "    cooccurrence = pd.DataFrame(0, index=top_skills_list, columns=top_skills_list)\n",
        "    \n",
        "    for skills_list in df_clean['skills']:\n",
        "        # For each pair of skills in the list\n",
        "        for i, skill1 in enumerate(skills_list):\n",
        "            if skill1 in top_skills_list:\n",
        "                for skill2 in skills_list[i+1:]:\n",
        "                    if skill2 in top_skills_list:\n",
        "                        cooccurrence.loc[skill1, skill2] += 1\n",
        "                        cooccurrence.loc[skill2, skill1] += 1\n",
        "    \n",
        "    # Create heatmap\n",
        "    fig, ax = plt.subplots(figsize=(14, 12))\n",
        "    sns.heatmap(cooccurrence, annot=True, fmt='d', cmap='YlOrRd', \n",
        "                square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
        "    ax.set_title('Skill Co-occurrence Matrix - How Often Skills Appear Together', \n",
        "                 fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../visualizations/skill_cooccurrence.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"‚úÖ Co-occurrence heatmap created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Machine Learning - Job Clustering <a id='clustering'></a>\n",
        "\n",
        "Use unsupervised learning to group similar jobs based on their requirements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for clustering\n",
        "if 'description' in df_clean.columns:\n",
        "    print(\"ü§ñ Preparing data for machine learning clustering...\\n\")\n",
        "    \n",
        "    # Filter out empty descriptions\n",
        "    df_ml = df_clean[df_clean['description'].str.len() > 50].copy()\n",
        "    \n",
        "    # Create TF-IDF features from job descriptions\n",
        "    print(\"üìä Creating TF-IDF features...\")\n",
        "    tfidf = TfidfVectorizer(\n",
        "        max_features=100,\n",
        "        stop_words='english',\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=5,\n",
        "        max_df=0.8\n",
        "    )\n",
        "    \n",
        "    tfidf_matrix = tfidf.fit_transform(df_ml['description'])\n",
        "    print(f\"‚úÖ TF-IDF matrix created: {tfidf_matrix.shape}\")\n",
        "    \n",
        "    # Determine optimal number of clusters using elbow method\n",
        "    print(\"\\nüîç Finding optimal number of clusters...\")\n",
        "    inertias = []\n",
        "    K_range = range(2, 11)\n",
        "    \n",
        "    for k in K_range:\n",
        "        kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        kmeans_temp.fit(tfidf_matrix)\n",
        "        inertias.append(kmeans_temp.inertia_)\n",
        "    \n",
        "    # Plot elbow curve\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
        "    ax.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
        "    ax.set_ylabel('Inertia', fontsize=12)\n",
        "    ax.set_title('Elbow Method - Finding Optimal k', fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../visualizations/elbow_curve.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"‚úÖ Elbow curve generated!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform K-Means clustering\n",
        "if 'description' in df_clean.columns:\n",
        "    print(\"\\nüéØ Performing K-Means clustering with k=5...\")\n",
        "    \n",
        "    # Apply KMeans\n",
        "    optimal_k = 5  # You can adjust this based on the elbow curve\n",
        "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "    df_ml['cluster'] = kmeans.fit_predict(tfidf_matrix)\n",
        "    \n",
        "    print(f\"‚úÖ Clustering complete!\")\n",
        "    print(f\"\\nCluster distribution:\")\n",
        "    print(df_ml['cluster'].value_counts().sort_index())\n",
        "    \n",
        "    # Analyze each cluster\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"CLUSTER ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for cluster_id in range(optimal_k):\n",
        "        cluster_jobs = df_ml[df_ml['cluster'] == cluster_id]\n",
        "        \n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"CLUSTER {cluster_id} ({len(cluster_jobs)} jobs)\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # Most common job titles\n",
        "        if 'job_title_clean' in cluster_jobs.columns:\n",
        "            print(\"\\nTop Job Titles:\")\n",
        "            print(cluster_jobs['job_title_clean'].value_counts().head(5).to_string())\n",
        "        \n",
        "        # Most common skills\n",
        "        if 'skills' in cluster_jobs.columns:\n",
        "            cluster_skills = [skill for skills_list in cluster_jobs['skills'] for skill in skills_list]\n",
        "            if len(cluster_skills) > 0:\n",
        "                print(\"\\nTop Skills:\")\n",
        "                top_cluster_skills = Counter(cluster_skills).most_common(10)\n",
        "                for skill, count in top_cluster_skills:\n",
        "                    print(f\"  ‚Ä¢ {skill}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize clusters using PCA\n",
        "if 'cluster' in df_ml.columns:\n",
        "    print(\"\\nüìä Visualizing clusters with PCA...\")\n",
        "    \n",
        "    # Reduce dimensions to 2D using PCA\n",
        "    pca = PCA(n_components=2, random_state=42)\n",
        "    tfidf_dense = tfidf_matrix.toarray()\n",
        "    principal_components = pca.fit_transform(tfidf_dense)\n",
        "    \n",
        "    # Create scatter plot\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    scatter = ax.scatter(\n",
        "        principal_components[:, 0],\n",
        "        principal_components[:, 1],\n",
        "        c=df_ml['cluster'],\n",
        "        cmap='viridis',\n",
        "        alpha=0.6,\n",
        "        s=50,\n",
        "        edgecolors='black',\n",
        "        linewidth=0.5\n",
        "    )\n",
        "    \n",
        "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)', fontsize=12)\n",
        "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)', fontsize=12)\n",
        "    ax.set_title('Job Clustering Visualization (PCA)', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(scatter, ax=ax)\n",
        "    cbar.set_label('Cluster', fontsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../visualizations/job_clusters.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"‚úÖ Cluster visualization created!\")\n",
        "    print(f\"üìä PCA explained variance: {sum(pca.explained_variance_ratio_):.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Key Insights & Recommendations <a id='insights'></a>\n",
        "\n",
        "Summarize findings and provide actionable recommendations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive insights report\n",
        "print(\"=\" * 80)\n",
        "print(\"KEY INSIGHTS & FINDINGS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. Dataset overview\n",
        "print(f\"\\nüìä DATASET OVERVIEW\")\n",
        "print(f\"{'‚îÄ'*80}\")\n",
        "print(f\"Total Job Postings Analyzed: {len(df_clean):,}\")\n",
        "print(f\"Unique Job Titles: {df_clean['job_title'].nunique() if 'job_title' in df_clean.columns else 'N/A'}\")\n",
        "print(f\"Unique Companies: {df_clean['company'].nunique() if 'company' in df_clean.columns else 'N/A'}\")\n",
        "print(f\"Unique Locations: {df_clean['location'].nunique() if 'location' in df_clean.columns else 'N/A'}\")\n",
        "\n",
        "# 2. Most in-demand skills\n",
        "if 'skills' in df_clean.columns and len(all_skills) > 0:\n",
        "    print(f\"\\nüî• TOP 10 MOST IN-DEMAND SKILLS\")\n",
        "    print(f\"{'‚îÄ'*80}\")\n",
        "    top_10_skills = skill_counts.most_common(10)\n",
        "    for idx, (skill, count) in enumerate(top_10_skills, 1):\n",
        "        percentage = (count / len(df_clean)) * 100\n",
        "        print(f\"{idx:2d}. {skill:30s} - {count:5,} jobs ({percentage:.1f}%)\")\n",
        "\n",
        "# 3. Most common job titles\n",
        "if 'job_title_clean' in df_clean.columns:\n",
        "    print(f\"\\nüíº TOP 10 MOST COMMON JOB TITLES\")\n",
        "    print(f\"{'‚îÄ'*80}\")\n",
        "    top_10_titles = df_clean['job_title_clean'].value_counts().head(10)\n",
        "    for idx, (title, count) in enumerate(top_10_titles.items(), 1):\n",
        "        percentage = (count / len(df_clean)) * 100\n",
        "        print(f\"{idx:2d}. {title:35s} - {count:5,} jobs ({percentage:.1f}%)\")\n",
        "\n",
        "# 4. Location insights\n",
        "if 'location' in df_clean.columns:\n",
        "    print(f\"\\nüåç TOP 5 LOCATIONS\")\n",
        "    print(f\"{'‚îÄ'*80}\")\n",
        "    top_5_locations = df_clean['location'].value_counts().head(5)\n",
        "    for idx, (location, count) in enumerate(top_5_locations.items(), 1):\n",
        "        percentage = (count / len(df_clean)) * 100\n",
        "        print(f\"{idx}. {location:40s} - {count:5,} jobs ({percentage:.1f}%)\")\n",
        "\n",
        "# 5. Experience level distribution\n",
        "if 'experience_level' in df_clean.columns:\n",
        "    print(f\"\\nüìà EXPERIENCE LEVEL DISTRIBUTION\")\n",
        "    print(f\"{'‚îÄ'*80}\")\n",
        "    exp_dist = df_clean['experience_level'].value_counts()\n",
        "    for level, count in exp_dist.items():\n",
        "        percentage = (count / len(df_clean)) * 100\n",
        "        print(f\"‚Ä¢ {level:20s} - {count:5,} jobs ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìù Strategic Recommendations\n",
        "\n",
        "Based on the analysis, here are actionable recommendations:\n",
        "\n",
        "#### For Job Seekers:\n",
        "\n",
        "1. **Focus on Core Skills**: Prioritize learning the top 10 most in-demand skills identified in this analysis\n",
        "2. **Combination Matters**: Skills often appear together (see co-occurrence matrix) - build complementary skill sets\n",
        "3. **Location Strategy**: Consider opportunities in cities with highest job demand\n",
        "4. **Experience Level**: Understand typical requirements for your target roles\n",
        "\n",
        "#### For Employers:\n",
        "\n",
        "1. **Competitive Requirements**: Align job postings with market standards for required skills\n",
        "2. **Skill Trends**: Stay updated on emerging technologies showing growth\n",
        "3. **Talent Pool**: Consider skills that are frequently bundled together when hiring\n",
        "\n",
        "#### For Career Development:\n",
        "\n",
        "1. **Upskilling Path**: Create a learning roadmap based on skill frequency and co-occurrence\n",
        "2. **Role Transitions**: Use cluster analysis to identify similar roles that match your skillset\n",
        "3. **Market Positioning**: Understand how your skills align with current market demands\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save cleaned dataset for future use\n",
        "output_path = '../data/processed/job_market_clean.csv'\n",
        "df_clean.to_csv(output_path, index=False)\n",
        "print(f\"‚úÖ Cleaned dataset saved to: {output_path}\")\n",
        "\n",
        "# Save skills analysis\n",
        "if 'skills' in df_clean.columns and len(all_skills) > 0:\n",
        "    skills_df = pd.DataFrame(skill_counts.most_common(50), columns=['Skill', 'Frequency'])\n",
        "    skills_df.to_csv('../data/processed/top_skills.csv', index=False)\n",
        "    print(f\"‚úÖ Top skills saved to: ../data/processed/top_skills.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ANALYSIS COMPLETE! üéâ\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nAll visualizations have been saved to: ../visualizations/\")\n",
        "print(\"\\nVisualization files created:\")\n",
        "print(\"  ‚Ä¢ top_job_titles.png\")\n",
        "print(\"  ‚Ä¢ top_locations.png\")\n",
        "print(\"  ‚Ä¢ experience_distribution.png\")\n",
        "print(\"  ‚Ä¢ top_skills.png\")\n",
        "print(\"  ‚Ä¢ skills_by_category.png\")\n",
        "print(\"  ‚Ä¢ skills_wordcloud.png\")\n",
        "print(\"  ‚Ä¢ skills_by_job_title.png\")\n",
        "print(\"  ‚Ä¢ skill_cooccurrence.png\")\n",
        "print(\"  ‚Ä¢ elbow_curve.png\")\n",
        "print(\"  ‚Ä¢ job_clusters.png\")\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üöÄ Next Steps\n",
        "\n",
        "### To enhance this project further:\n",
        "\n",
        "1. **Time Series Analysis**: If your dataset has date information, analyze trends over time\n",
        "2. **Salary Analysis**: If salary data is available, analyze compensation patterns by role, location, and experience\n",
        "3. **Interactive Dashboard**: Create a Streamlit dashboard for interactive exploration\n",
        "4. **Web Scraping**: Collect real-time data from job sites for fresh insights\n",
        "5. **Predictive Models**: Build models to predict job category or salary based on description\n",
        "6. **Geographic Visualization**: Create maps showing job distribution across regions\n",
        "7. **Skill Gap Analysis**: Compare required vs. available skills in the market\n",
        "\n",
        "### Resources for Learning:\n",
        "- **Python for Data Analysis** by Wes McKinney\n",
        "- **Kaggle Learn**: Free data science courses\n",
        "- **Towards Data Science**: Articles on job market analysis\n",
        "- **LinkedIn Learning**: Courses on data visualization and NLP\n",
        "\n",
        "---\n",
        "\n",
        "## üì¨ Contact & Portfolio\n",
        "\n",
        "**Project Repository**: [Add your GitHub link]  \n",
        "**LinkedIn**: [Add your LinkedIn profile]  \n",
        "**Email**: [Your email]\n",
        "\n",
        "---\n",
        "\n",
        "*This project demonstrates proficiency in: Python, Data Analysis, Data Visualization, NLP, Machine Learning, and Business Intelligence*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
